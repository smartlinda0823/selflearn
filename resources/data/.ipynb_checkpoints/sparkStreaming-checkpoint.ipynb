{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row,SQLContext\n",
    "from google.cloud import bigquery\n",
    "from os import listdir\n",
    "import sys\n",
    "import requests\n",
    "import time\n",
    "import subprocess\n",
    "import re,os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # global variables\n",
    "bucket = \"streamprocessing-group12\" \n",
    "# output_directory_hashtags = 'gs://{}/hadoop/tmp/bigquery/pyspark_output/hashtagsCount'.format(bucket)\n",
    "# output_directory_wordcount = 'gs://{}/hadoop/tmp/bigquery/pyspark_output/wordcount'.format(bucket)\n",
    "output_directory_hashtags = './hashtags'\n",
    "output_directory_wordcount = './wordcount'\n",
    "    # output table and columns name\n",
    "# output_dataset = 'result'                     #the name of dataset in BigQuery\n",
    "output_table_hashtags = 'hashtags'\n",
    "columns_name_hashtags = ['hashtags', 'count']\n",
    "output_table_wordcount = 'wordcount'\n",
    "columns_name_wordcount = ['word', 'count', 'time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # parameter\n",
    "IP = 'localhost'    # ip port\n",
    "PORT = 9001       # port\n",
    "STREAMTIME = 960         # time that the streaming process runs\n",
    "WORD = ['covid-19', 'stayathome', 'hero', 'nursing']     #the words you should filter and do word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def saveToStorage(rdd, output_table, output_directory, columns_name, mode):\n",
    "    \"\"\"\n",
    "    Save each RDD in this DStream to google storage\n",
    "    Args:\n",
    "        rdd: input rdd\n",
    "        output_directory: output directory\n",
    "        columns_name: columns name of dataframe\n",
    "        mode: mode = \"overwirte\", overwirte the file\n",
    "              mode = \"append\", append data to the end of file\n",
    "    \"\"\"\n",
    "    file_name = '{}.csv'.format(output_table) \n",
    "#     file_exists = os.path.isfile(file_name)\n",
    "#     csvWriter = csv.writer(csvFile)   \n",
    "#     csvFile = open(output_table, 'a')   \n",
    "#     if not file_exists:\n",
    "#             csvWriter.writerow(columns_name)\n",
    "            \n",
    "    if not rdd.isEmpty():\n",
    "        (rdd.toDF( columns_name ) \\\n",
    "        .repartition(1).write.option(\"header\", \"true\").csv(path = output_directory, mode=mode))\n",
    "        \n",
    "    CSV_File = [file for file in listdir(output_directory) if file.endswith('.csv')][0]\n",
    "    if CSV_File != file_name:\n",
    "        old_file = os.path.join(output_directory, CSV_File)\n",
    "        new_file = os.path.join(output_directory, file_name)\n",
    "        os.rename(old_file, new_file)\n",
    "\n",
    "def saveToBigQuery(sc, output_dataset, output_table, directory):\n",
    "    \"\"\"\n",
    "    Put temp streaming json files in google storage to google BigQuery\n",
    "    and clean the output files in google storage\n",
    "    \"\"\"\n",
    "    files = directory + '/part-*'\n",
    "    subprocess.check_call(\n",
    "        'bq load --source_format NEWLINE_DELIMITED_JSON '\n",
    "        '--replace '\n",
    "        '--autodetect '\n",
    "        '{dataset}.{table} {files}'.format(\n",
    "            dataset=output_dataset, table=output_table, files=files\n",
    "        ).split())\n",
    "    output_path = sc._jvm.org.apache.hadoop.fs.Path(directory)\n",
    "    output_path.getFileSystem(sc._jsc.hadoopConfiguration()).delete(\n",
    "        output_path, True)\n",
    "\n",
    "# def saveTolocal(sc, columns_name, output_table):\n",
    "#     df = SQLContext.createDataFrame(sc, columns_name)\n",
    "#     df.coalesce(1).write.format(output_table).options(header='true').save('./csv_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_count (newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtagCount(words):\n",
    "    \"\"\"\n",
    "    Calculate the accumulated hashtags count sum from the beginning of the stream\n",
    "    and sort it by descending order of the count.\n",
    "    Ignore case sensitivity when counting the hashtags: \"#Ab\" and \"#ab\" is considered to be a same hashtag\n",
    "    Args:\n",
    "        dstream(DStream): stream of real time tweets\n",
    "    Returns:\n",
    "        DStream Object with inner structure (hashtag, count)\n",
    "    \"\"\"\n",
    "    # Filter out the word that is hashtags, and map them into (hashtag, 1)\n",
    "    hashtags = words.filter(lambda a: '#' in a and a != '#').map(lambda x: (x.lower(), 1))\n",
    "    # sum the count of current DStream state and previous state\n",
    "    hashtags_totals = hashtags.updateStateByKey(aggregate_count)\n",
    "    sorted_hashtags = hashtags_totals.transform(lambda rdd: rdd.sortBy(lambda x: -x[1]))\n",
    "    return sorted_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCount(words):\n",
    "    \"\"\"\n",
    "    Calculte the count of 5 sepcial words in 120 seconds for every 120 seconds (window no overlap)\n",
    "    Args:\n",
    "        dstream(DStream): stream of real time tweets\n",
    "    Returns:\n",
    "        DStream Object with inner structure (word, count, time)\n",
    "    \"\"\"\n",
    "    # Reduce last 120 seconds of data, every 120 seconds\n",
    "    wordnew = words.filter(lambda x:x.lower() in WORD).map(lambda x: (x.lower(),1))\n",
    "    windowedWordCounts = wordnew.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 120, 120)\n",
    "    DStreamwithTime = windowedWordCounts.transform(lambda time, rdd: rdd.map(lambda x:(x[0], x[1], str(time))))\n",
    "    return DStreamwithTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Spark settings\n",
    "conf = SparkConf()\n",
    "conf.setMaster('local[2]')\n",
    "conf.setAppName(\"TwitterStreamApp\")\n",
    "    \n",
    "    # create spark context with the above configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "    # create sql context, used for saving rdd\n",
    "sql_context = SQLContext(sc)\n",
    "\n",
    "    # create the Streaming Context from the above spark context with batch interval size 5 seconds\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "    # setting a checkpoint to allow RDD recovery\n",
    "ssc.checkpoint(\"~/checkpoint_TwitterApp\")\n",
    "\n",
    "    # read data from port 9001\n",
    "dataStream = ssc.socketTextStream(IP, PORT)\n",
    "# dataStream.pprint()\n",
    "    \n",
    "words = dataStream.flatMap(lambda line: line.split(\" \"))\n",
    "    # calculate the accumulated hashtags count sum from the beginning of the stream\n",
    "topTags = hashtagCount(words)\n",
    "# topTags.pprint()\n",
    "    # Calculte the word count during each time period 60s\n",
    "CountResult = wordCount(words)\n",
    "# CountResult.pprint()\n",
    "\n",
    "    # save hashtags count and word count to storage\n",
    "topTags.foreachRDD(lambda a: saveToStorage(a, output_table_hashtags, output_directory_hashtags, columns_name_hashtags, \"overwrite\"))\n",
    "CountResult.foreachRDD(lambda b: saveToStorage(b, output_table_wordcount, output_directory_wordcount,columns_name_wordcount,\"overwrite\"))\n",
    "\n",
    "    # start streaming process, wait for 600s and then stop.\n",
    "ssc.start()\n",
    "time.sleep(STREAMTIME)\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "\n",
    "# saveToBigQuery(sc, output_dataset, output_table_hashtags, output_directory_hashtags)\n",
    "# saveToBigQuery(sc, output_dataset, output_table_wordcount, output_directory_wordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
